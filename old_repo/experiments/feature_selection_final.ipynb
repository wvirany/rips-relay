{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "print('Current conda environment:', os.environ['CONDA_DEFAULT_ENV'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "\n",
    "from crem.crem import grow_mol, mutate_mol\n",
    "crem_db = '../crem_db/crem_db2.5.db'\n",
    "\n",
    "import mols2grid\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdFingerprintGenerator, CanonSmiles, Draw, MolFromSmiles, PandasTools\n",
    "from rdkit.Chem.rdmolops import RDKFingerprint\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.rdFMCS import FindMCS\n",
    "from rdkit.DataStructs.cDataStructs import BulkTanimotoSimilarity\n",
    "import useful_rdkit_utils as uru\n",
    "\n",
    "import prolif as plf\n",
    "\n",
    "import safe as sf\n",
    "import datamol as dm\n",
    "\n",
    "import mols2grid\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import itertools \n",
    "from coati.generative.coati_purifications import embed_smiles\n",
    "from coati.models.io.coati import load_e3gnn_smiles_clip_e2e\n",
    "from coati.models.simple_coati2.io import load_coati2\n",
    "\n",
    "\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem.Scaffolds import rdScaffoldNetwork\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn settings for visualizations\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#f7f9fc\",\n",
    "    \"figure.facecolor\": \"#f7f9fc\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "/\n",
    "default_palette = 'tab10'\n",
    "\n",
    "sns.set(rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load initial ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = '2zdt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mol = Chem.MolFromMolFile(f\"data/docking/{pdb}_ligand.sdf\")\n",
    "initial = Chem.MolToSmiles(initial_mol)\n",
    "\n",
    "MolsToGridImage([Chem.MolFromSmiles(initial)], subImgSize=(600, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Molecular Formula of Initial Ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SMILES to a molecule object\n",
    "mol = Chem.MolFromSmiles(initial)\n",
    "\n",
    "# Get the molecular formula\n",
    "formula = Chem.rdMolDescriptors.CalcMolFormula(mol)\n",
    "\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction fingerprint for reference molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_MOL_FILEPATH = f\"data/docking/{pdb}_ligand.sdf\"\n",
    "PDB_FILEPATH = f\"data/docking/{pdb}.pdb\"\n",
    "\n",
    "fp = plf.Fingerprint()\n",
    "\n",
    "mol = Chem.MolFromPDBFile(PDB_FILEPATH, removeHs=False)\n",
    "prot = plf.Molecule(mol)\n",
    "suppl = plf.sdf_supplier(REF_MOL_FILEPATH)\n",
    "fp.run_from_iterable(suppl, prot, progress=True)\n",
    "df_ifp = fp.to_dataframe()\n",
    "df_ifp.columns = df_ifp.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ifp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction fingerprint functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ifp_similarity computes the intersection of IMFs between a generated molecule and a reference fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifp_similarity(ref_mol_ifp, df_ifp, df):\n",
    "    ## Rename columns\n",
    "    df_ifp.columns = [' '.join(col) if isinstance(col, tuple) else col for col in df_ifp.columns]\n",
    "    ref_mol_ifp.columns = [' '.join(col) if isinstance(col, tuple) else col for col in ref_mol_ifp.columns]\n",
    "    \n",
    "\n",
    "    intersections = []\n",
    "    weighted_intersections = []\n",
    "\n",
    "    #iterate over the rows\n",
    "    for index, row in df_ifp.iterrows():\n",
    "        count=0\n",
    "        weighted_count = 0\n",
    "        #iterate over all columns\n",
    "        for col_name in df_ifp.columns:\n",
    "            if col_name in ref_mol_ifp.columns and df_ifp[col_name][index]==ref_mol_ifp[col_name][0] and 'VdWContact' in col_name:\n",
    "                count += 1\n",
    "                weighted_count += 1\n",
    "            elif col_name in ref_mol_ifp.columns and df_ifp[col_name][index]==ref_mol_ifp[col_name][0] and 'Hydrophobic' in col_name:\n",
    "                count += 1\n",
    "                weighted_count += 2\n",
    "            elif col_name in ref_mol_ifp.columns and df_ifp[col_name][index]==ref_mol_ifp[col_name][0] and 'HBAcceptor' in col_name:\n",
    "                count += 1\n",
    "                weighted_count += 3\n",
    "            elif col_name in ref_mol_ifp.columns and df_ifp[col_name][index]==ref_mol_ifp[col_name][0] and 'Anionic' in col_name or 'Cationic' in col_name:\n",
    "                count += 1\n",
    "                weighted_count += 4\n",
    "        \n",
    "        intersections.append(count)\n",
    "        weighted_intersections.append(weighted_count)\n",
    "                \n",
    "    df['IFP Intersection'] = intersections\n",
    "    df['Weighted IFP Intersection'] = weighted_intersections\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute_features counts total number of interactions, number of interactions per IMF and weighted number of interactions for generated molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df, ifp):\n",
    "\n",
    "    cols = ifp.columns\n",
    "\n",
    "    num_cols = len(ifp.columns)\n",
    "\n",
    "    data = {'mol_id' : [],\n",
    "            'num_interactions' : [],\n",
    "            'weighted_interactions' : [],\n",
    "            'num_VdW' : [],\n",
    "            'num_hydrophobic' : [],\n",
    "            'num_HBAcceptor' : [],\n",
    "            'num_ionic' : []}\n",
    "\n",
    "    for index, row in ifp.iterrows():\n",
    "\n",
    "        weighted_interactions = 0\n",
    "        num_VdW = 0\n",
    "        num_hydrophobic = 0\n",
    "        num_HBAcceptor = 0\n",
    "        num_ionic = 0\n",
    "        \n",
    "        data['mol_id'].append(row['ID'][0])\n",
    "        data['num_interactions'].append(row[:-1].sum())\n",
    "\n",
    "        for value in cols:\n",
    "            \n",
    "            if value[1] == 'VdWContact':\n",
    "                weighted_interactions += 1 * row[value]\n",
    "                num_VdW += 1 * row[value]\n",
    "            elif value[1] == 'Hydrophobic':\n",
    "                weighted_interactions += 2 * row[value]\n",
    "                num_hydrophobic += 1 * row[value]\n",
    "            elif value[1] == 'HBAcceptor':\n",
    "                weighted_interactions += 3 * row[value]\n",
    "                num_HBAcceptor += 1 * row[value]\n",
    "            elif value[1] == 'Anionic' or value[1] == 'Cationic':\n",
    "                weighted_interactions += 4 * row[value]\n",
    "                num_ionic += 1 * row[value]\n",
    "\n",
    "        data['weighted_interactions'].append(weighted_interactions)\n",
    "        data['num_VdW'].append(num_VdW)\n",
    "        data['num_hydrophobic'].append(num_hydrophobic)\n",
    "        data['num_HBAcceptor'].append(num_HBAcceptor)\n",
    "        data['num_ionic'].append(num_ionic)\n",
    "\n",
    "            \n",
    "\n",
    "    features = pd.DataFrame(data)\n",
    "\n",
    "    df = df.merge(features[['mol_id', 'num_interactions', 'weighted_interactions', 'num_VdW', 'num_hydrophobic', 'num_HBAcceptor', 'num_ionic']], left_on='ID', right_on='mol_id', how='left')\n",
    "\n",
    "    df = df.drop(['mol_id'], axis=1).sort_values(['Docking score'], ascending=True)\n",
    "\n",
    "    df.dropna(axis=0, subset=['Docking score'], inplace=True)\n",
    "    df['num_interactions'].fillna(0, inplace=True)\n",
    "    df['weighted_interactions'].fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fingerprint(ifp):\n",
    "\n",
    "    sns.set(rc = {'figure.figsize':(15,8)})\n",
    "    ax = sns.heatmap(ifp,cmap=sns.cm.rocket_r)\n",
    "    ax.set_ylabel(\"Molecule\")\n",
    "    ax.set_xlabel(\"Protein Interaction\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function will be used to remove duplicates produced by the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(df, inchi_names):\n",
    "    \"\"\"\n",
    "    Removes rows with repeated 'inchi' and 'Model' values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with columns 'inchi' and 'Model'\n",
    "    inchi_names (list): List of inchi names to check for repeats\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with repeats removed\n",
    "    \"\"\"\n",
    "    # Iterate over each inchi name in inchi_names\n",
    "    for inchi_name in inchi_names:\n",
    "        # Get the subset of the DataFrame for the current inchi_name\n",
    "        df_inchi = df[df['inchi'] == inchi_name]\n",
    "        \n",
    "        # Find duplicates based on 'inchi' and 'Model' columns\n",
    "        duplicates = df_inchi[df_inchi.duplicated(subset=['inchi', 'Model'], keep='first')]\n",
    "        \n",
    "        # Drop the duplicate rows from the original DataFrame\n",
    "        df = df.drop(duplicates.index)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will be used to identify duplicates across different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all molecules seemed to have a 3D conformers, so we make sure that they do with the ensure 3d conformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_3d_conformer(mol):\n",
    "    \"\"\"\n",
    "    Ensure that the RDKit molecule object has a 3D conformation.\n",
    "    \n",
    "    Parameters:\n",
    "    - mol: RDKit molecule object\n",
    "    \n",
    "    Returns:\n",
    "    - mol: RDKit molecule object with a 3D conformation\n",
    "    \"\"\"\n",
    "    if not mol.GetNumConformers():\n",
    "        # Add hydrogens if not already present\n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        # Generate 3D conformation\n",
    "        result = AllChem.EmbedMolecule(mol)\n",
    "        if result == -1:  # -1 means embedding failed\n",
    "            raise ValueError(\"Failed to generate 3D conformation for the molecule.\")\n",
    "        \n",
    "        # Optimize the 3D conformation\n",
    "        AllChem.UFFOptimizeMolecule(mol)\n",
    "        \n",
    "    return mol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function calculates RMSD between two molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmsd(mol1, mol2):\n",
    "    \"\"\"\n",
    "    Calculate RMSD between two RDKit molecule objects with 3D conformations.\n",
    "    \n",
    "    Parameters:\n",
    "    - mol1: RDKit molecule object\n",
    "    - mol2: RDKit molecule object\n",
    "    \n",
    "    Returns:\n",
    "    - RMSD value (float)\n",
    "    \"\"\"\n",
    "    # Ensure both molecules have 3D conformers\n",
    "    mol1 = ensure_3d_conformer(mol1)\n",
    "    mol2 = ensure_3d_conformer(mol2)\n",
    "    \n",
    "    if not mol1.GetNumConformers() or not mol2.GetNumConformers():\n",
    "        raise ValueError(\"One or both molecules could not be embedded or optimized.\")\n",
    "\n",
    "    # Calculate RMSD\n",
    "    rms = AllChem.GetBestRMS(mol1, mol2)\n",
    "    \n",
    "    return rms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect duplicates based on same inchi and RMSD value of less than or equal to 3 (most molecules do and threshold can be adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes as input an repeated inchi value and returns a dataframe of molecules that have RMSD <=0.3 and their corresponding RoMOL\n",
    "def duplicate_detector(df):\n",
    "    \n",
    "    duplicates_list=[]\n",
    "    for a,b in itertools.combinations(df.ROMol,2):\n",
    "\n",
    "        #Calculate average RMSD since it changes at every run\n",
    "        RMSD_list=[]\n",
    "        for _ in range(10):\n",
    "            RMSD = calculate_rmsd(a,b)\n",
    "            RMSD_list+=[RMSD]\n",
    "        # Filter out non-numeric values\n",
    "        RMSD_list = [num for num in RMSD_list if isinstance(num, (int, float))]\n",
    "\n",
    "        # Calculate the average\n",
    "        if RMSD_list:\n",
    "            RMSD_average = sum(RMSD_list) / len(RMSD_list)\n",
    "        else:\n",
    "            RMSD_average = None\n",
    "\n",
    "        rms = RMSD_average\n",
    "\n",
    "        #Filter at 1 Amstrong\n",
    "        if rms <=3:\n",
    "            # Get SMILES String\n",
    "            mol_a = df[df['ROMol'] == a].iloc[0,0]\n",
    "            mol_b = df[df['ROMol'] == b].iloc[0,0]\n",
    "            # Get model\n",
    "            model_a_index = df[df['ROMol'] == a].index[0]\n",
    "            model_a= df.at[model_a_index, 'Model']\n",
    "            \n",
    "            model_b_index = df[df['ROMol'] == b].index[0]\n",
    "            model_b= df.at[model_b_index, 'Model']\n",
    "\n",
    "            #Add ROMol\n",
    "            duplicates_list.append((mol_a, model_a, a, mol_b, model_b, b, rms))\n",
    "    column_names = ['Smiles_1', 'model_1', 'ROMol_1', 'Smiles_2', 'model_2', 'ROMol_2', 'Average_RMSD']\n",
    "    df_new=pd.DataFrame(duplicates_list, columns=column_names)\n",
    "\n",
    "    return df_new   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function checks repeated molecules from all of the repeated inchi values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes as input dataframe with all data and a list of repeated inchi names and returns a dataframe with duplicates\n",
    "def process_inchi_names(model_df, inchi_names):\n",
    "    \"\"\"Process a list of InChI names and concatenate results.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for inchi_name in inchi_names:\n",
    "        # Filter the DataFrame for the current InChI name\n",
    "        df_filtered = model_df.query(f'inchi == \"{inchi_name}\"')\n",
    "        \n",
    "        # Apply the duplicate_detector function\n",
    "        df_duplicates = duplicate_detector(df_filtered)\n",
    "        \n",
    "        # Append result if it's not empty\n",
    "        if not df_duplicates.empty:\n",
    "            all_results.append(df_duplicates)\n",
    "    \n",
    "    # Concatenate all results into a single DataFrame, if there are any\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        # Return an empty DataFrame with the same columns if no results\n",
    "        column_names = ['Smiles_1', 'model_1', 'ROMol_1', 'Smiles_2', 'model_2', 'ROMol_2', 'Average_RMSD']\n",
    "        final_df = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function removes generated molecules that have more than one fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_broken_molecules(df):\n",
    "\n",
    "    smiles = df['SMILES'].to_list()\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles]\n",
    "\n",
    "    df['ROMol'] = mols\n",
    "\n",
    "    df.dropna(subset=['ROMol'], inplace=True)\n",
    "\n",
    "    df['num_frags'] = df.ROMol.apply(uru.count_fragments)\n",
    "\n",
    "    df_single_frag = df.query(\"num_frags == 1\").copy()\n",
    "\n",
    "    return df_single_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Analogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'reinvent'\n",
    "\n",
    "arg1 = '--model'\n",
    "arg2 = '--sample'\n",
    "arg3 = '--dock'\n",
    "arg4 = '--pdb'\n",
    "\n",
    "args = ['python3', 'generate_analogs.py',\n",
    "        arg1, model,\n",
    "        arg2, '200',\n",
    "        arg3,\n",
    "        arg4, pdb]\n",
    "\n",
    "# Change directory to generate analogs with python script\n",
    "%cd ..\n",
    "\n",
    "subprocess.run(args,\n",
    "               stdout=subprocess.DEVNULL,\n",
    "               stderr=subprocess.STDOUT)\n",
    "        \n",
    "# Change directory back to that of the current notebook\n",
    "%cd experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FILEPATH = f'experiments/data/{model}_dataframe.csv'\n",
    "IFP_FILEPATH = f'experiments/data/{model}_ifp.csv'\n",
    "\n",
    "df_reinvent = pd.read_csv(DF_FILEPATH, index_col=0)\n",
    "\n",
    "ifp_reinvent = pd.read_csv(IFP_FILEPATH, header=[0, 1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute length to check that no molecules are being filtered by metric computation\n",
    "len(df_reinvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of IMFs\n",
    "df_reinvent = compute_features(df_reinvent, ifp_reinvent)\n",
    "# Compare IMFs to initial fragment\n",
    "df_reinvent = ifp_similarity(df_ifp, ifp_reinvent, df_reinvent)\n",
    "\n",
    "df_reinvent.drop(['Input_SMILES', 'Prior', 'Tanimoto'], axis=1, inplace=True)\n",
    "df_reinvent['Model'] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates and Broken Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with mol object information for each molecule\n",
    "df_reinvent['ROMol']=[Chem.MolFromSmiles(x) for x in df_reinvent['SMILES'].values]\n",
    "df_reinvent['inchi'] = df_reinvent.ROMol.apply(Chem.MolToInchiKey)\n",
    "\n",
    "# Count repeated InChI\n",
    "value_counts_reinvent=df_reinvent.inchi.value_counts()\n",
    "filtered_counts_reinvent = value_counts_reinvent[value_counts_reinvent > 1]\n",
    "\n",
    "# Get values for InChI\n",
    "inchi_names_reinvent=filtered_counts_reinvent.index.tolist()\n",
    "filtered_counts_reinvent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reinvent = remove_repeats(df_reinvent, inchi_names_reinvent)\n",
    "df_reinvent = remove_broken_molecules(df_reinvent)\n",
    "df_reinvent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CReM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'crem'\n",
    "\n",
    "arg1 = '--model'\n",
    "arg2 = '--sample'\n",
    "arg3 = '--dock'\n",
    "arg4 = '--pdb'\n",
    "\n",
    "args = ['python3', 'generate_analogs.py',\n",
    "        arg1, model,\n",
    "        arg2, '200',\n",
    "        arg3,\n",
    "        arg4, pdb]\n",
    "\n",
    "# Change directory to generate analogs with python script\n",
    "%cd ..\n",
    "\n",
    "subprocess.run(args,\n",
    "               stdout=subprocess.DEVNULL,\n",
    "               stderr=subprocess.STDOUT)\n",
    "        \n",
    "# Change directory back to that of the current notebook\n",
    "%cd experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FILEPATH = f'experiments/data/{model}_dataframe.csv'\n",
    "IFP_FILEPATH = f'experiments/data/{model}_ifp.csv'\n",
    "\n",
    "df_crem = pd.read_csv(DF_FILEPATH, index_col=0)\n",
    "\n",
    "ifp_crem = pd.read_csv(IFP_FILEPATH, header=[0, 1], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute length to check that no molecules are being filtered by metric computation\n",
    "len(df_crem), len(ifp_crem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of IMFs\n",
    "df_crem = compute_features(df_crem, ifp_crem)\n",
    "len(df_crem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: compute_features drops molecules that are not able to dock, i.e, do not have a docking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IMFs to initial fragment\n",
    "df_crem = ifp_similarity(df_ifp, ifp_crem, df_crem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates and Broken Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with mol object information for each molecule\n",
    "df_crem['ROMol']=[Chem.MolFromSmiles(x) for x in df_crem['SMILES'].values]\n",
    "df_crem['inchi'] = df_crem.ROMol.apply(Chem.MolToInchiKey)\n",
    "\n",
    "# Count repeated InChI\n",
    "value_counts_crem=df_crem.inchi.value_counts()\n",
    "filtered_counts_crem = value_counts_crem[value_counts_crem > 1]\n",
    "\n",
    "# Get values for InChI\n",
    "inchi_names_crem=filtered_counts_crem.index.tolist()\n",
    "filtered_counts_crem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crem=remove_repeats(df_crem, inchi_names_crem)\n",
    "df_crem= remove_broken_molecules(df_crem)\n",
    "df_crem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe that contains all generated molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat((df_reinvent, df_crem))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'coati'\n",
    "\n",
    "arg1 = '--model'\n",
    "arg2 = '--sample'\n",
    "arg3 = '--dock'\n",
    "arg4 = '--pdb'\n",
    "\n",
    "args = ['python3', 'generate_analogs.py',\n",
    "        arg1, model,\n",
    "        arg2, '200',\n",
    "        arg3,\n",
    "        arg4, pdb]\n",
    "\n",
    "# Change directory to generate analogs with python script\n",
    "%cd ..\n",
    "\n",
    "subprocess.run(args,\n",
    "               stdout=subprocess.DEVNULL,\n",
    "               stderr=subprocess.STDOUT)\n",
    "        \n",
    "# Change directory back to that of the current notebook\n",
    "%cd experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FILEPATH = f'experiments/data/{model}_dataframe.csv'\n",
    "IFP_FILEPATH = f'experiments/data/{model}_ifp.csv'\n",
    "\n",
    "df_coati = pd.read_csv(DF_FILEPATH, index_col=0)\n",
    "\n",
    "ifp_coati = pd.read_csv(IFP_FILEPATH, header=[0, 1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of IMFs\n",
    "df_coati = compute_features(df_coati, ifp_coati)\n",
    "df_coati = ifp_similarity(df_ifp, ifp_coati, df_coati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize df_coati\n",
    "df_coati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates and Broken Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with mol object information for each molecule\n",
    "df_coati['ROMol']=[Chem.MolFromSmiles(x) for x in df_coati['SMILES'].values]\n",
    "df_coati['inchi'] = df_coati.ROMol.apply(Chem.MolToInchiKey)\n",
    "\n",
    "# Count repeated InChI\n",
    "value_counts_coati=df_coati.inchi.value_counts()\n",
    "filtered_counts_coati = value_counts_coati[value_counts_coati > 1]\n",
    "\n",
    "#Get values of InChi\n",
    "inchi_names_coati=filtered_counts_coati.index.tolist()\n",
    "filtered_counts_coati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coati = remove_repeats(df_coati, inchi_names_coati)\n",
    "df_coati = remove_broken_molecules(df_coati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat((model_df, df_coati))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'safe'\n",
    "\n",
    "arg1 = '--model'\n",
    "arg2 = '--sample'\n",
    "arg3 = '--dock'\n",
    "arg4 = '--pdb'\n",
    "\n",
    "args = ['python3', 'generate_analogs.py',\n",
    "        arg1, model,\n",
    "        arg2, '200',\n",
    "        arg3,\n",
    "        arg4, pdb]\n",
    "\n",
    "# Change directory to generate analogs with python script\n",
    "%cd ..\n",
    "\n",
    "subprocess.run(args,\n",
    "               stdout=subprocess.DEVNULL,\n",
    "               stderr=subprocess.STDOUT)\n",
    "        \n",
    "# Change directory back to that of the current notebook\n",
    "%cd experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FILEPATH = f'experiments/data/{model}_dataframe.csv'\n",
    "IFP_FILEPATH = f'data/{model}_ifp.csv'\n",
    "\n",
    "\n",
    "df_safe = pd.read_csv(DF_FILEPATH, index_col=0)\n",
    "\n",
    "ifp_safe = pd.read_csv(IFP_FILEPATH, header=[0, 1], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute IMFs\n",
    "df_safe = compute_features(df_safe, ifp_safe)\n",
    "#Compare IMFs of reference molecule\n",
    "df_safe = ifp_similarity(df_ifp, ifp_safe, df_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize df\n",
    "df_safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates and Broken Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with mol object information for each molecule\n",
    "df_safe['ROMol']=[Chem.MolFromSmiles(x) for x in df_safe['SMILES'].values]\n",
    "df_safe['inchi'] = df_safe.ROMol.apply(Chem.MolToInchiKey)\n",
    "\n",
    "# Count repeated InChI\n",
    "value_counts_safe=df_safe.inchi.value_counts()\n",
    "filtered_counts_safe = value_counts_safe[value_counts_safe > 1]\n",
    "\n",
    "# Get values for InChI\n",
    "inchi_names_safe=filtered_counts_safe.index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_safe = remove_repeats(df_safe, inchi_names_safe)\n",
    "df_safe = remove_broken_molecules(df_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat((model_df, df_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Metrics with MolScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molscore import MolScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = model_df['SMILES'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MolScore(model_name='mol2mol', task_config='molscore/feature_selection.json')\n",
    "\n",
    "scores = ms.score(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'experiments/data/goodness_scoring_data/2024_08_07_mol2mol_feature_selection_18_21_40/iterations/000001_scores.csv'  \n",
    "\n",
    "molscore_df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molscore_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: even though the indexes in model_df and df do not match, the information in each row does match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molscore_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop categorical features and redudant features\n",
    "\n",
    "# Got rid of SA score and desc_PenLogP  because all entries are zero\n",
    "# Got rid of QED as in ecompasses several other features\n",
    "\n",
    "X = molscore_df.drop(['smiles', 'model', 'task', 'step',\n",
    "            'batch_idx', 'absolute_time',\n",
    "            'valid', 'valid_score', 'unique',\n",
    "            'occurrences','dice_Cmpd1_Sim', 'tanimoto_Cmpd1_Sim', \n",
    "            'amean', 'filter',\n",
    "            'score_time', 'raw_valid_score','desc_MolecularFormula', 'desc_SAscore', 'desc_PenLogP', 'desc_QED'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ifp_intersection']=model_df['IFP Intersection'].values \n",
    "X['Docking score'] = model_df['Docking score'].values\n",
    "X['num_interactions'] = model_df['num_interactions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_ifp_intersection = model_df['Weighted IFP Intersection'].values\n",
    "# X['ifp_intersection']=model_df['IFP Intersection'].values \n",
    "# X['Weighted IFP Similarity'] = weighted_ifp_intersection / model_df['weighted_interactions'].values\n",
    "# X['IFP Similarity'] = weighted_ifp_intersection / model_df['weighted_interactions'].values\n",
    "# X['Docking score'] = model_df['Docking score'].values\n",
    "# X['num_interactions'] = model_df['num_interactions'].values\n",
    "# X['weighted_interactions'] = model_df['weighted_interactions'].values\n",
    "# X['Interaction Weight Ratio'] = model_df['weighted_interactions'].values / model_df['num_interactions'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molscore_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {\n",
    "    'desc_Bertz' : 'Synthetic Complexity',\n",
    "    'interaction weight ratio' : 'Avg Interaction Strength',\n",
    "    'Weighted IFP Similarity' : 'Weighted Interaction Similarity',\n",
    "    'RAScore_pred_proba' : 'Synthetic Accessibility',\n",
    "    'desc_NumHeteroatoms' : '# Heteroatoms',\n",
    "    'desc_HeavyAtomMolWt': \"Heavy Atom MolWt\", \n",
    "    'desc_NumHAcceptors': '# HAcceptors', \n",
    "    'desc_NumHDonors':\"#HDonors\",\n",
    "    'desc_NumRotatableBonds': '# Rotatable Bonds',\n",
    "    'desc_NumAromaticRings': '# Aromatic Rings', \n",
    "    'desc_NumAliphaticRings': 'Number Aliphatic Rings', \n",
    "    'desc_RingCount': 'Ring Count',\n",
    "    'desc_TPSA': 'TPSA', \n",
    "    'desc_FormalCharge': 'Formal Charge',\n",
    "    'desc_CLogP': 'CLogP',\n",
    "    'desc_MolWt': 'MolWt', \n",
    "    'desc_HeavyAtomCount': 'Heavy Atom Count',\n",
    "    'desc_MaxConsecutiveRotatableBonds': 'Max Consecutive Rotatable Bonds',\n",
    "    'tanimoto_Sim': 'Tanimoto Sim',\n",
    "    'dice_Sim': 'Dice Sim',\n",
    "    'desc_FlourineCount':'Fluorine Count'\n",
    "    }\n",
    "\n",
    "X.rename(columns=column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save X df for animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "animation_df = X.copy()\n",
    "animation_df['Model'] =  model_df['Model'].values\n",
    "animation_df.to_csv('data/animation_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = X.copy()\n",
    "\n",
    "#Apply absolute value to values of docking score before normalizing\n",
    "X_normalized['Docking score'] = X_normalized['Docking score'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing each column using min-max scaler\n",
    "for column in X_normalized.columns:\n",
    "   if X_normalized[column].max() - X_normalized[column].min() == 0:\n",
    "      X_normalized[column]== X_normalized[column].max()\n",
    "   else:\n",
    "      X_normalized[column] = (X_normalized[column] - X_normalized[column].min()) / (X_normalized[column].max() - X_normalized[column].min()) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append Model column to X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indices to match the order\n",
    "df_features = model_df.copy()\n",
    "df_features=df_features.reset_index(drop=True)\n",
    "# df_model_reset = df_model.reset_index(drop=True)\n",
    "\n",
    "# Append the 'model' column to the features DataFrame\n",
    "X_normalized = pd.concat([X_normalized, df_features['Model']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-Test for Feature Importance in Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing: For a fixed feature, are the means significantly different across four models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical target to numerical values\n",
    "X_ANOVA=X_normalized.copy()\n",
    "X_ANOVA['Model'] = X_ANOVA['Model'].astype('category').cat.codes\n",
    "\n",
    "# Extract features and target\n",
    "X_new = X_ANOVA.drop('Model', axis=1)\n",
    "y = X_ANOVA['Model']\n",
    "\n",
    "# Perform ANOVA F-test\n",
    "f_values, p_values = f_classif(X_new, y)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_ANOVA = pd.DataFrame({'Feature': X_new.columns, 'F-value': f_values, 'p-value': p_values})\n",
    "\n",
    "# Sort the results by F-value in descending order\n",
    "results_ANOVA = results_ANOVA.sort_values(by='F-value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Variance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = X_ANOVA.var()\n",
    "print(\"Feature Variances:\\n\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_correlation=X_normalized.copy().drop('Model', axis=1)\n",
    "correlation_matrix = X_correlation.corr()\n",
    "X_correlation.corr().style.background_gradient(cmap='coolwarm', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify which features are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes as input a correlation matrix and a threshold, selects features with correlation greater or equal to\n",
    "# threshol and returns 3-tuples of the form (feat1, feat2, correlation)\n",
    "\n",
    "def get_highly_correlated_pairs(corr_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Returns pairs of features with correlation greater than or equal to the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    corr_matrix (pd.DataFrame): Correlation matrix of features\n",
    "    threshold (float): Threshold for correlation value\n",
    "    \n",
    "    Returns:\n",
    "    List[Tuple[str, str, float]]: List of 3-tuples (feat1, feat2, correlation_score)\n",
    "    \"\"\"\n",
    "    correlated_pairs = []\n",
    "    \n",
    "    # Iterate over the correlation matrix\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) >= threshold:\n",
    "                feat1 = corr_matrix.columns[i]\n",
    "                feat2 = corr_matrix.columns[j]\n",
    "                corr_score = corr_matrix.iloc[i, j]\n",
    "                correlated_pairs.append((feat1, feat2, corr_score))\n",
    "                \n",
    "    return correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the threshold to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "highly_correlated_pairs = get_highly_correlated_pairs(correlation_matrix, threshold)\n",
    "highly_correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of highly correlated features with the following criterion:\n",
    "1. Feature is correlated with three or more features.\n",
    "2. If a feature is correlated with two or one other feature, then the features with least importance in the ANOVA F-test are filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions returns takes as input a list of 3-tuples of the form (feat1, feat2, corr_score) and a df with the ranked features according to ANOVA F-test. The function returns the features that must be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filter_correlated_features(correlations, ranked_df):\n",
    "    # Count the occurrences of each feature in the correlations list\n",
    "    feature_counts = Counter([feat for tup in correlations for feat in tup[:2]])\n",
    "\n",
    "    # Identify features correlated with three or more others\n",
    "    high_corr_features = set([feat for feat, count in feature_counts.items() if count >= 3])\n",
    "\n",
    "    # Identify features correlated with one or two others\n",
    "    low_corr_features = [tup for tup in correlations if feature_counts[tup[0]] < 3 and feature_counts[tup[1]] < 3]\n",
    "\n",
    "    # Filter out the least important features based on the ANOVA F-test ranking\n",
    "    filtered_features = set()\n",
    "    for feat1, feat2, _ in low_corr_features:\n",
    "        feat1_rank = ranked_df.loc[feat1, 'ANOVA_F_Rank']\n",
    "        feat2_rank = ranked_df.loc[feat2, 'ANOVA_F_Rank']\n",
    "        if feat1_rank > feat2_rank:\n",
    "            filtered_features.add(feat1)\n",
    "        else:\n",
    "            filtered_features.add(feat2)\n",
    "\n",
    "    # Combine the high_corr_features and filtered_features to identify all features to remove\n",
    "    features_to_remove = high_corr_features.union(filtered_features)\n",
    "    \n",
    "    return features_to_remove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered features: synthetic complexity, tanimoto_sim, desc_HeavyAtomMolWt, desc_HeavyAtomCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with the the least correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RF_corr_analysis = X_normalized.copy().drop(['Dice Sim', 'Synthetic Complexity', 'MolWt', 'Heavy Atom MolWt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RF_corr_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Experiment with Correlation Analysis, Stratefied Sampling and k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using X_normalized, we use X_RF_corr_analysis, which is the dataframe after correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RF_corr_analysis.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X_RF = X_RF_corr_analysis.copy().drop('Model', axis=1)\n",
    "y_RF = df_features['Model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for cross-validation\n",
    "n_splits = 5\n",
    "scores_strat = []\n",
    "importances = np.zeros(X_RF.shape[1])\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_RF, y_RF)):\n",
    "    X_train, X_test = X_RF.iloc[train_index], X_RF.iloc[test_index]\n",
    "    y_train, y_test = y_RF.iloc[train_index], y_RF.iloc[test_index]\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    scores_strat.append(score)\n",
    "    \n",
    "    # Accumulate feature importances\n",
    "    importances += model.feature_importances_\n",
    "    \n",
    "    # Debug: Print fold number and score\n",
    "    print(f'Fold {fold + 1}: Accuracy = {score}')\n",
    "\n",
    "# Average the importances across all folds\n",
    "importances /= n_splits\n",
    "\n",
    "# Print the average prediction score\n",
    "average_score = np.mean(scores_strat)\n",
    "print(f'Average Prediction Score: {average_score}')\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df_strat = pd.DataFrame({\n",
    "    'Feature': X_RF.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "importance_explained_rf_strat = feature_importance_df_strat['Importance'].iloc[:6].sum()\n",
    "print(f'Differences Explained by Top 5 Features: {importance_explained_rf_strat}')\n",
    "print(feature_importance_df_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Correlation Analysis, Scaffold Split and 5-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract the scaffold of each molecule using RDKit's MurckoScaffold and MakeScaffoldGeneric.\n",
    "2. Create a dictionary where keys are scaffolds and values are lists of molecule indices.\n",
    "3. Split the dataset into training and testing sets ensuring that molecules with the same scaffold are not split between the training and testing sets and maintaining the stratified sampling based on the molecule's model type.\n",
    "4. Train a Random Forest model using the stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaffold = X_RF_corr_analysis.copy()\n",
    "df_scaffold['Model']=X_normalized['Model']\n",
    "df_scaffold['smiles']=molscore_df['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list with smiles strings\n",
    "smiles_df_scaffold = df_scaffold['smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with generic scaffolds\n",
    "generic_scaffold_list = []\n",
    "for smile in smiles_df_scaffold:\n",
    "    scaffold = Chem.MolToSmiles(MurckoScaffold.MakeScaffoldGeneric(Chem.MolFromSmiles(smile)))\n",
    "    generic_scaffold_list += [scaffold]\n",
    "\n",
    "df_scaffold['Generic_Scaffold'] = generic_scaffold_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_scaff = df_scaffold.drop(columns=['Model', 'Generic_Scaffold', 'smiles'])\n",
    "y_scaff = df_scaffold['Model']\n",
    "scaffold = df_scaffold['Generic_Scaffold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map scaffolds to indices\n",
    "scaffold_dict = defaultdict(list)\n",
    "for idx, s in enumerate(scaffold):\n",
    "    scaffold_dict[s].append(idx)\n",
    "\n",
    "# Create a dictionary to map scaffolds to their labels\n",
    "scaffold_labels = {}\n",
    "for s, indices in scaffold_dict.items():\n",
    "    scaffold_labels[s] = y_scaff.iloc[indices].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function ensures:\n",
    "1. Repeated scaffolds are not added to both train and test sets\n",
    "2. Each split has an even class distribution, where the each class is the model type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stratified splitting by scaffold\n",
    "def stratified_scaffold_split(X, y, scaffold_dict, scaffold_labels, n_splits=5, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    scaffolds = list(scaffold_dict.keys())\n",
    "    np.random.shuffle(scaffolds)\n",
    "    fold_size = len(scaffolds) // n_splits\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        test_scaffolds = set(scaffolds[i * fold_size: (i + 1) * fold_size])\n",
    "        train_scaffolds = set(scaffolds) - test_scaffolds\n",
    "        \n",
    "        train_indices = [idx for s in train_scaffolds for idx in scaffold_dict[s]]\n",
    "        test_indices = [idx for s in test_scaffolds for idx in scaffold_dict[s]]\n",
    "        \n",
    "        # Ensure the stratified split maintains class proportions within scaffolds\n",
    "        y_train = y.iloc[train_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "        \n",
    "        if len(set(y_test)) > 0 and len(set(y_train)) > 0:\n",
    "            yield train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for cross-validation\n",
    "n_splits = 5\n",
    "scores_scaffold = []\n",
    "importances = np.zeros(X_scaff.shape[1])  # Initialize with the number of features in X_scaff\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "valid_splits = 0\n",
    "for fold, (train_index, test_index) in enumerate(stratified_scaffold_split(X_scaff, y_scaff, scaffold_dict, scaffold_labels, n_splits=n_splits, random_state=42)):\n",
    "    valid_splits += 1\n",
    "    X_train, X_test = X_scaff.iloc[train_index], X_scaff.iloc[test_index]\n",
    "    y_train, y_test = y_scaff.iloc[train_index], y_scaff.iloc[test_index]\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    scores_scaffold.append(score)\n",
    "    \n",
    "    # Check if the feature importances match the expected shape\n",
    "    if len(model.feature_importances_) == importances.shape[0]:\n",
    "        # Accumulate feature importances\n",
    "        importances += model.feature_importances_\n",
    "    else:\n",
    "        raise ValueError(f\"Shape mismatch: expected {importances.shape[0]} but got {len(model.feature_importances_)}\")\n",
    "    \n",
    "    # Debug: Print fold number and score\n",
    "    print(f'Fold {fold + 1}: Accuracy = {score}')\n",
    "\n",
    "# Check if we got valid splits\n",
    "if valid_splits == 0:\n",
    "    raise ValueError(\"No valid train-test splits found. Please check the splitting function.\")\n",
    "\n",
    "# Average the feature importances\n",
    "importances /= valid_splits\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_scaffold_df = pd.DataFrame({\n",
    "    'Feature': X_scaff.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance explained by the top 5 features\n",
    "importance_explained_rf_scaffold = feature_importance_scaffold_df['Importance'].iloc[:5].sum()\n",
    "\n",
    "# Print the results\n",
    "print(f'Prediction Scores: {scores_scaffold}')\n",
    "print(f'Average Prediction Score: {np.mean(scores_scaffold)}')\n",
    "print(f'Differences Explained by Top 5 Features: {importance_explained_rf_scaffold}')\n",
    "print(f'Feature Importances:\\n{feature_importance_scaffold_df}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Random Forest Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_scaffold = feature_importance_scaffold_df.head(10)\n",
    "average_pred_scaffold = np.mean(scores_scaffold)\n",
    "feature_importance_strat = feature_importance_df_strat.head(10)\n",
    "average_pred_strat = np.mean(scores_strat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "# List of dataframes and titles\n",
    "dataframes = [feature_importance_scaffold, feature_importance_strat]\n",
    "average_prediction = [average_pred_scaffold , average_pred_strat]\n",
    "top_6_explain = [importance_explained_rf_scaffold, importance_explained_rf_strat]\n",
    "\n",
    "titles = ['Feature Importances with Scaffold Stratified Sampling and 5-Fold CV',\n",
    "           'Feature Importances with Stratified Sampling and 5-Fold CV']\n",
    "\n",
    "# Calculate the max y-value for setting the same y-axis scale\n",
    "max_value = max([df['Importance'].max() for df in dataframes]) + 0.01\n",
    "min_value = min([df['Importance'].min() for df in dataframes]) - 0.005\n",
    "\n",
    "# Create a figure and axis objects with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 11))\n",
    "fig.suptitle('Random Forest Experiments Comparison',fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Iterate over dataframes and axes\n",
    "for i, (df, title) in enumerate(zip(dataframes, titles)):\n",
    "    sns.barplot(x='Feature', y='Importance', palette='colorblind', data=df, ax=axs[i])\n",
    "    axs[i].set_ylim(min_value, max_value)  # Set the same y-axis scale\n",
    "    axs[i].tick_params(axis='x', rotation=90)  # Rotate x-ticks\n",
    "    axs[i].set_title(title)\n",
    "\n",
    "    # Add custom legend with average prediction and cumulative weight of top 5 features\n",
    "    ave_pred = average_prediction[i] * 100\n",
    "    top_6_expl = top_6_explain[i]\n",
    "\n",
    "    # Create custom legend lines\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='w', label=f'Avg. Prediction: {ave_pred:.3f} %'),\n",
    "        Line2D([0], [0], color='w', label=f'Cumulative Weight Top 6: {top_6_expl:.3f}')\n",
    "    ]\n",
    "\n",
    "    # Add legend to the plot\n",
    "    axs[i].legend(handles=legend_elements, loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis with RF Correlation Analysis, Scaffold Split and 5-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next natural question to ask is: how sensible are these results (feature's weights) with respect to the input parameters (initial distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate how sensitive the feature importances and model performance are by training the model on different bootstrap samples of the data. For this analysis, we use the dataframe after correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X_sen_ana = X_RF_corr_analysis.copy().drop('Model', axis=1)\n",
    "y = df_features['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define number of bootstraps and number of splits for cross-validation\n",
    "num_bootstraps = 100\n",
    "num_splits = 5\n",
    "\n",
    "# Initialize storage for feature importances and rankings\n",
    "importances = np.zeros((num_bootstraps, X_sen_ana.shape[1]))\n",
    "all_predictions = []\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=num_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_bootstraps):\n",
    "    X_resample, y_resample = resample(X_sen_ana, y, random_state=i)\n",
    "\n",
    "    fold_importances = np.zeros((num_splits, X_sen_ana.shape[1]))\n",
    "    fold_predictions = []\n",
    "\n",
    "    valid_splits = 0\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(stratified_scaffold_split(X_resample, y_resample, scaffold_dict, scaffold_labels, n_splits=num_splits, random_state=42)):\n",
    "        valid_splits += 1\n",
    "        X_train, X_test = X_resample.iloc[train_idx], X_resample.iloc[test_idx]\n",
    "        y_train, y_test = y_resample.iloc[train_idx], y_resample.iloc[test_idx]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=250, max_depth=None, random_state=1)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_test)\n",
    "        fold_predictions.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        fold_importances[fold_idx, :] = rf.feature_importances_\n",
    "\n",
    "    if valid_splits == 0:\n",
    "        raise ValueError(\"No valid train-test splits found. Please check the splitting function.\")\n",
    "\n",
    "    importances[i, :] = np.mean(fold_importances, axis=0)\n",
    "    all_predictions.append(np.mean(fold_predictions))\n",
    "\n",
    "# Calculate mean and standard deviation of importances\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "std_importances = np.std(importances, axis=0)\n",
    "\n",
    "# Calculate mean and standard deviation of predictions\n",
    "mean_prediction = np.mean(all_predictions)\n",
    "std_prediction = np.std(all_predictions)\n",
    "\n",
    "print(\"Mean Importances:\\n\", mean_importances)\n",
    "print(\"Standard Deviation of Importances:\\n\", std_importances)\n",
    "print(\"Mean Prediction:\\n\", mean_prediction)\n",
    "print(\"Standard Deviation of Prediction:\\n\", std_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_sen_ana.columns,\n",
    "    'Mean Importance': mean_importances,\n",
    "    'Std Importance': std_importances\n",
    "}).sort_values(by='Mean Importance', ascending=False)\n",
    "\n",
    "# Select top 10 features\n",
    "top_10_features = feature_importance_df.head(10)\n",
    "# Create a figure and axis objects with subplots\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot the histogram\n",
    "sns.set_palette(\"colorblind\")\n",
    "bars = ax.bar(top_10_features['Feature'], top_10_features['Mean Importance'], yerr=top_10_features['Std Importance'], capsize=5, color=sns.color_palette(\"colorblind\"))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Average Importance')\n",
    "ax.set_title('Average Top 10 Feature Importances', fontsize=16, fontweight='bold',  y=1.05)\n",
    "ax.set_xticklabels(top_10_features['Feature'], rotation=90)\n",
    "\n",
    "# Create legend elements\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='w', markersize=10, label=f'Avg. Prediction: {mean_prediction*100:.3f} %' ),\n",
    "    Line2D([0], [0], color='w', markersize=10, label=f'STD of Prediction: {std_prediction*100:.3f} %'),\n",
    "    Line2D([0], [0], color='w', markersize=10, label=f'Explained by Top 5:  62.409 %')\n",
    "]\n",
    "\n",
    "# Add legend to the plot\n",
    "ax.legend(handles=legend_elements, loc='upper right', frameon=False)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Importances:\\n\", mean_importances)\n",
    "print(\"Standard Deviation of Importances:\\n\", std_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RF_corr_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing top distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look at distributions of the 10 features with highest average weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_importances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of the top 10 features\n",
    "top_10_indices = np.argsort(mean_importances)[-10:][::-1]\n",
    "\n",
    "# Extract importances and rankings for top 10 features\n",
    "top_10_importances = importances[:, top_10_indices]\n",
    "top_10_rankings = rankings[:, top_10_indices]\n",
    "\n",
    "top_10_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_importances\n",
    "X_sen_ana.columns[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the max x and y limits for the histograms\n",
    "max_importance = np.max(top_10_importances)\n",
    "max_frequency = 0\n",
    "\n",
    "min_importance = np.min(top_10_importances)\n",
    "\n",
    "for i in range(10):\n",
    "    counts, _ = np.histogram(top_10_importances[:, i], bins=30)\n",
    "    max_frequency = max(max_frequency, np.max(counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.figure(figsize=(20, 9))\n",
    "\n",
    "for i in range(10):\n",
    "    mean = mean_importances[top_10_indices[i]]\n",
    "    std = std_importances[top_10_indices[i]]\n",
    "    \n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    # Plot histogram\n",
    "    ax.hist(top_10_importances[:, i], bins=30, edgecolor='k', alpha=0.7)\n",
    "    \n",
    "    # Add title and legend with custom lines and labels\n",
    "    ax.set_title(f'{X_sen_ana.columns[top_10_indices[i]]} Importance', pad=20)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(min_importance - 0.01, max_importance + 0.01)\n",
    "    ax.set_ylim(0, max_frequency + 2)\n",
    "\n",
    "    # Create empty handles for legend\n",
    "    mean_handle = Line2D([0], [0], color='none', marker='o', markerfacecolor='black', markersize=6, linestyle='None')\n",
    "    std_handle = Line2D([0], [0], color='none', marker='o', markerfacecolor='black', markersize=6, linestyle='None')\n",
    "    \n",
    "      # Add the legend\n",
    "    ax.legend(handles=[mean_handle, std_handle], labels=[\n",
    "        f'Mean: {mean:.3f}', \n",
    "        f'Std: {std:.3f}'\n",
    "    ], loc='upper left')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit titles and labels\n",
    "plt.suptitle('Distribution of Feature Importance for Top 10 Features', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Silhouette score to find the optimal subset of three features that are best at clustering the four models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for Optimal Clustering Analysis and create df with top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FILEPATH = f'data/animation_df.csv'\n",
    "X =  pd.read_csv(DF_FILEPATH, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = X[['CLogP', 'Heavy Atom Count', '# Rotatable Bonds', 'Formal Charge',\n",
    "        'Tanimoto Sim', 'Docking score', 'Max Consecutive Rotatable Bonds', 'ifp_intersection', 'TPSA', 'Synthetic Accessibility', 'Model']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_cluster['model_encoded'] = le.fit_transform(df_cluster['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "feature_columns = df_cluster.columns.difference(['Model', 'model_encoded'])\n",
    "\n",
    "# Generate all combinations of three features\n",
    "combinations = list(itertools.combinations(feature_columns, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions computes the combination of three features with the highest Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_combinations(df, combinations):\n",
    "    best_score = -1\n",
    "    best_combination = None\n",
    "    \n",
    "    for combo in combinations:\n",
    "        # Compute silhouette score\n",
    "        X = df[list(combo)]\n",
    "        try:\n",
    "            score = silhouette_score(X, df['model_encoded'], metric='euclidean')\n",
    "        except ValueError:\n",
    "            score = -1  # If not enough samples per cluster\n",
    "\n",
    "        print(f\"Combination: {combo}, Silhouette Score: {score:.2f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_combination = combo\n",
    "    \n",
    "    return best_combination, best_score\n",
    "\n",
    "# Evaluate combinations\n",
    "best_comb, best_score = evaluate_feature_combinations(df_cluster, combinations)\n",
    "print(f\"Best Combination: {best_comb}, Silhouette Score: {best_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinvent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
