{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing Distributions of Generated Molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "print('Current conda environment:', os.environ['CONDA_DEFAULT_ENV'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # set gpu\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Working directory:', cwd)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, PandasTools\n",
    "from rdkit.Chem.Draw import MolsToGridImage, MolsMatrixToGridImage\n",
    "from rdkit import DataStructs\n",
    "import useful_rdkit_utils as uru\n",
    "\n",
    "import pickle\n",
    "\n",
    "from molscore import MolScore\n",
    "from moleval.metrics.metrics import GetMetrics\n",
    "\n",
    "from fcd import get_fcd\n",
    "\n",
    "import medchem as mc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Seaborn settings for visualizations\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#f7f9fc\",\n",
    "    \"figure.facecolor\": \"#f7f9fc\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "\n",
    "default_palette = 'tab10'\n",
    "\n",
    "sns.set(rc=rc)\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to improve readability\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = []\n",
    "\n",
    "f = open(\"data/fragments.smi\", \"r\")\n",
    "\n",
    "for i in range(1, 51):\n",
    "    mol = f.readline()\n",
    "    fragments.append(mol[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['reinvent', 'crem', 'coati', 'safe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# reinvent_distributions, crem_distributions, coati_distributions, safe_distributions = [], [], [], []\n",
    "\n",
    "# for fragment in fragments:\n",
    "#     for model in models:\n",
    "\n",
    "#         DF_FILEPATH = f'data/{model}_dataframe.csv'\n",
    "\n",
    "#         arg1 = '--model'\n",
    "#         arg2 = '--input_frag'\n",
    "#         arg3 = '--sample'\n",
    "\n",
    "#         args = ['python3', 'generate_analogs.py',\n",
    "#                 arg1, model,\n",
    "#                 arg2, fragment,\n",
    "#                 arg3, '200']\n",
    "\n",
    "#         # Change directory to generate analogs with python script\n",
    "#         %cd ..\n",
    "\n",
    "#         subprocess.run(args,\n",
    "#                     stdout=subprocess.DEVNULL,\n",
    "#                     stderr=subprocess.STDOUT)\n",
    "                \n",
    "#         # Change directory back to that of the current notebook\n",
    "#         %cd experiments\n",
    "\n",
    "#         df = pd.read_csv(DF_FILEPATH, index_col=0)\n",
    "\n",
    "#         df['Model'] = model\n",
    "\n",
    "#         if model == 'reinvent':\n",
    "#             reinvent_distributions.append(df)\n",
    "#         elif model == 'crem':\n",
    "#             crem_distributions.append(df)\n",
    "#         elif model == 'coati':\n",
    "#             coati_distributions.append(df)\n",
    "#         elif model == 'safe':\n",
    "#             safe_distributions.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'reinvent' : reinvent_distributions,\n",
    "#         'crem' : crem_distributions,\n",
    "#         'coati' : coati_distributions,\n",
    "#         'safe' : safe_distributions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('lists.pkl', 'wb') as file:\n",
    "#     pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lists.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinvent_distributions = data['reinvent']\n",
    "crem_distributions = data['crem']\n",
    "coati_distributions = data['coati']\n",
    "safe_distributions = data['safe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "\n",
    "    for i, df in enumerate(data[d]):\n",
    "\n",
    "        df['Input Fragment'] = fragments[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_int_vect_to_numpy(sparse_vect, length):\n",
    "    array = np.zeros(length, dtype=int)\n",
    "    for idx, value in sparse_vect.GetNonzeroElements().items():\n",
    "        array[idx] = value\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_odd_rings(df):\n",
    "\n",
    "    if df.empty: return df\n",
    "\n",
    "    ring_system_lookup = uru.RingSystemLookup.default()\n",
    "    df['ring_systems'] = df.SMILES.apply(ring_system_lookup.process_smiles)\n",
    "    df[['min_ring','min_freq']] = df.ring_systems.apply(uru.get_min_ring_frequency).to_list()\n",
    "\n",
    "    odd_rings = df['min_freq'] < 100\n",
    "\n",
    "    df['Odd rings'] = ~ odd_rings\n",
    "\n",
    "    df_good_rings = df[df['Odd rings']].drop(columns=['Odd rings'])\n",
    "    \n",
    "    return df_good_rings.loc[:, ['SMILES', 'Model', 'Input Fragment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_broken_molecles(df):\n",
    "\n",
    "    smiles = df['SMILES'].to_list()\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles]\n",
    "\n",
    "    df['ROMol'] = mols\n",
    "\n",
    "    df.dropna(subset=['ROMol'], inplace=True)\n",
    "\n",
    "    df['num_frags'] = df.ROMol.apply(uru.count_fragments)\n",
    "\n",
    "    df_single_frag = df.query(\"num_frags == 1\").copy()\n",
    "\n",
    "    return df_single_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fcd(fragment, smiles): \n",
    "    frag_mol = Chem.MolFromSmiles(fragment)\n",
    "    canon_frag = Chem.MolToSmiles(frag_mol)\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n",
    "    canon_smiles = [Chem.MolToSmiles(mol) for mol in mols]\n",
    "\n",
    "    fcd = get_fcd(canon_frag, canon_smiles)\n",
    "\n",
    "    return fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanimoto_similarity(smi_1, smi_2):\n",
    "    fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=2048,countSimulation=True)\n",
    "    mol_1 = Chem.MolFromSmiles(smi_1)\n",
    "    mol_2 = Chem.MolFromSmiles(smi_2)\n",
    "    \n",
    "    fp_1 = rdFingerprintGenerator.GetCountFPs([mol_1])[0]\n",
    "    fp_2 = rdFingerprintGenerator.GetCountFPs([mol_2])[0]\n",
    "        \n",
    "    return DataStructs.TanimotoSimilarity(fp_1, fp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(smi_1, smi_2):\n",
    "    fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=2048,countSimulation=True)\n",
    "    mol_1 = Chem.MolFromSmiles(smi_1)\n",
    "    mol_2 = Chem.MolFromSmiles(smi_2)\n",
    "    \n",
    "    fp1 = rdFingerprintGenerator.GetCountFPs([mol_1])[0]\n",
    "    fp2 = rdFingerprintGenerator.GetCountFPs([mol_2])[0]\n",
    "\n",
    "    A = sparse_int_vect_to_numpy(fp1, 2048)\n",
    "    B = sparse_int_vect_to_numpy(fp2, 2048)\n",
    "\n",
    "    return np.dot(A, B) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(fragment, smiles):\n",
    "\n",
    "    size = len(smiles)\n",
    "    snn = 0\n",
    "    avg_tanimoto = 0\n",
    "    avg_cosine = 0\n",
    "\n",
    "    for smile in smiles:\n",
    "\n",
    "        tanimoto = tanimoto_similarity(fragment, smile)\n",
    "        cosine = cosine_similarity(fragment, smile)\n",
    "\n",
    "        avg_tanimoto += tanimoto / size\n",
    "        avg_cosine += cosine / size\n",
    "\n",
    "        if tanimoto > snn: snn = tanimoto\n",
    "\n",
    "    return snn, avg_tanimoto, avg_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filters(df):\n",
    "\n",
    "    size = len(df)\n",
    "\n",
    "    smiles = df['SMILES'].to_list()\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles]\n",
    "\n",
    "    lipinski = [mc.rules.basic_rules.rule_of_five(mol) for mol in mols]\n",
    "\n",
    "    # Apply some default medchem filters\n",
    "    dundee = mc.functional.alert_filter(\n",
    "        mols=mols,\n",
    "        alerts=[\"Dundee\"],\n",
    "        n_jobs=-1,\n",
    "        progress=True,\n",
    "        return_idx=False,\n",
    "    )\n",
    "    \n",
    "    lily = mc.functional.lilly_demerit_filter(\n",
    "        mols=mols,\n",
    "        n_jobs=-1,\n",
    "        progress=True,\n",
    "        return_idx=False,\n",
    "    )\n",
    "\n",
    "    lipinski_score = np.sum(lipinski) / size\n",
    "    dundee_score = np.sum(dundee) / size\n",
    "    lily_score = np.sum(lily) / size\n",
    "\n",
    "    return lipinski_score, dundee_score, lily_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes distributions with zero molecules\n",
    "\n",
    "Removes invalid SMILES strings\n",
    "\n",
    "Removes molecules with multiple fragments\n",
    "\n",
    "Removes odd ring systems from distributions\n",
    "'''\n",
    "def preprocess_data(data):\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        for i, df in enumerate(data[d]):\n",
    "            \n",
    "            if len(df) < 30:\n",
    "                pass\n",
    "            else:\n",
    "                smiles_list = df['SMILES'].to_list()\n",
    "\n",
    "                valid_smiles = []\n",
    "                invalid_smiles = []\n",
    "\n",
    "                for smiles in smiles_list:\n",
    "                        \n",
    "                    try:\n",
    "                        molecule = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "                        if molecule is not None:\n",
    "                            valid_smiles.append(True)\n",
    "                        else:\n",
    "                            valid_smiles.append(False)\n",
    "                    except Exception as e:\n",
    "                        invalid_smiles.append(smiles)\n",
    "\n",
    "                df_1 = remove_broken_molecles(df)\n",
    "\n",
    "                df_2 = remove_odd_rings(df_1)\n",
    "\n",
    "                data[d][i] = df_2\n",
    "            \n",
    "        filtered_df_list = [df for df in data[d] if len(df) > 30]\n",
    "    \n",
    "        data[d] = filtered_df_list\n",
    "                \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(data):\n",
    "\n",
    "    metrics_df = pd.DataFrame()\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        for df in data[d]:\n",
    "            \n",
    "            size = len(df)\n",
    "\n",
    "            fragment = df['Input Fragment'].to_list()[0]\n",
    "            smiles = df['SMILES'].to_list()\n",
    "\n",
    "            MetricEngine = GetMetrics(\n",
    "                n_jobs=1,\n",
    "                device='cpu',\n",
    "                batch_size=512,\n",
    "                run_fcd=False,\n",
    "                train=fragment\n",
    "            )\n",
    "            metrics = MetricEngine.calculate(\n",
    "                smiles,\n",
    "                calc_valid=True,\n",
    "                calc_unique=True,\n",
    "                unique_k=10000,\n",
    "                se_k=1000,\n",
    "                sp_k=1000,\n",
    "                properties=True,\n",
    "            )\n",
    "\n",
    "            # Compute FreChet Distance\n",
    "\n",
    "            fcd = compute_fcd(fragment, smiles)\n",
    "\n",
    "            # Compute Single Nearest Neighbor Similarity\n",
    "\n",
    "            snn, avg_tanimoto, avg_cosine = compute_similarities(fragment, smiles)\n",
    "            lipinski_score, dundee_score, lily_score = process_filters(df)\n",
    "\n",
    "\n",
    "            metrics['FCD'] = fcd\n",
    "            metrics['SNN'] = snn\n",
    "            metrics['Avg Tanimoto'] = avg_tanimoto\n",
    "            metrics['Avg Cosine'] = avg_cosine\n",
    "            metrics['Lipinski Score'] = lipinski_score\n",
    "            metrics['Dundee Score'] = dundee_score\n",
    "            metrics['Lily Score'] = lily_score\n",
    "            metrics['Model'] = d\n",
    "\n",
    "            temp_df = pd.DataFrame(data=metrics, index=[0])\n",
    "\n",
    "            metrics_df = pd.concat((metrics_df, temp_df))\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_confidence_intervals(dataframes, models):\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for col in cols:\n",
    "        stats[col] = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        for col in cols:\n",
    "\n",
    "            samples = df[col].to_numpy()\n",
    "            mean = round(np.mean(samples),3)\n",
    "\n",
    "            confidence_interval = st.norm.interval(confidence=0.95,\n",
    "                                                loc=mean,\n",
    "                                                scale=st.sem(samples))\n",
    "            \n",
    "            int_length = round(.5 * (confidence_interval[1] - confidence_interval[0]), 3)\n",
    "\n",
    "            stats[col].append((mean, int_length))\n",
    "\n",
    "    stats_df = pd.DataFrame(data=stats)\n",
    "\n",
    "    stats_df['Model'] = models\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(metrics_df, cols):\n",
    "\n",
    "    X = metrics_df.loc[:, cols]\n",
    "    y = metrics_df['Model'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    enc.fit_transform(y)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100,\n",
    "                                class_weight='balanced',\n",
    "                                random_state=1)\n",
    "    \n",
    "    steps = [\n",
    "        ('scaler', StandardScaler()),  # Data preprocessing step\n",
    "        ('classifier', rf)  # Model step\n",
    "    ]\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_validate(rf, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    avg_precision, precision_std = scores['test_precision_macro'].mean(), scores['test_precision_macro'].std()\n",
    "    avg_recall, recall_std = scores['test_recall_macro'].mean(), scores['test_recall_macro'].std()\n",
    "    avg_f1, f1_std = scores['test_f1_macro'].mean(), scores['test_f1_macro'].std()\n",
    "\n",
    "    return [(avg_precision, precision_std), (avg_recall, recall_std), (avg_f1, f1_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(features, feature_importance):\n",
    "\n",
    "    d = {'Features': features, 'Weights': feature_importance}\n",
    "\n",
    "    rf_features = pd.DataFrame(data=d).sort_values(['Weights'], ascending=False, ignore_index=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.barplot(x = 'Features',\n",
    "                y = 'Weights',\n",
    "                data = rf_features,\n",
    "                palette=default_palette,\n",
    "                ax=ax)\n",
    "\n",
    "    # plt.title('Ranking of Random Forest Features')\n",
    "    ax.set_xlabel('Features', fontsize=20)\n",
    "    ax.set_ylabel('Feature Importance', labelpad=25, fontsize=20)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Distribution Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(data)\n",
    "\n",
    "reinvent_distributions = data['reinvent']\n",
    "crem_distributions = data['crem']\n",
    "coati_distributions = data['coati']\n",
    "safe_distributions = data['safe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['safe'].pop(8)\n",
    "# data['safe'].pop(7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# metrics_df = compute_metrics(data)\n",
    "\n",
    "# metrics_df.to_csv('data/metrics_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv('data/metrics_df', index_col=0)\n",
    "\n",
    "metrics_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['Avg_Filter_Score'] = (metrics_df['Lipinski Score'] + metrics_df['Dundee Score'] + metrics_df['Lily Score']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['IntDiv1', 'IntDiv2', 'SEDiv', 'SPDiv', 'ScaffDiv', 'FG', 'RS', 'Purchasable_ZINC20', 'FCD', 'SNN', 'Avg Tanimoto', 'Avg Cosine', 'Filters', 'Lipinski Score', 'Dundee Score', 'Lily Score', 'Avg_Filter_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[['Model'] + cols].groupby(\"Model\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[['Model'] + cols].groupby(\"Model\").std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    " * Uniqueness: Interestingly, COATI and CReM generate distributions which have a uniqueness score of 1, whereas REINVENT and SAFE seem to be generating more duplicates (with SAFE generating the most). It is also interesting to note that the standard deviation of uniqueness for SAFE distributions is much higher than the rest, indicating a high sensitivity for this metric to the input fragment.\n",
    "\n",
    " * Diversity: CReM tends to generate the most diverse distributions, whereas SAFE seems to generate the least diverse distributions. All models seem to have fairly consistent diversity metrics; however, it is interesting to note the high standard deviation in sphere exclusion diversity score for CReM.\n",
    "\n",
    " * Purchasability: Interestingly, CReM and REINVENT tend to generate purchasable molecules at a higher rate than the rest of the models, while SAFE has an average purchasability rate of 0 across all distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinvent_df = metrics_df.loc[metrics_df['Model'] == 'reinvent']\n",
    "crem_df = metrics_df.loc[metrics_df['Model'] == 'crem']\n",
    "coati_df = metrics_df.loc[metrics_df['Model'] == 'coati']\n",
    "safe_df = metrics_df.loc[metrics_df['Model'] == 'safe']\n",
    "\n",
    "dataframes = [reinvent_df, crem_df, coati_df, safe_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals = construct_confidence_intervals(dataframes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(metrics_df.isna(), aspect=\"auto\", interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.xlabel(\"Column Number\")\n",
    "plt.ylabel(\"Sample Number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distributions (of distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=len(cols),\n",
    "                        ncols=3,\n",
    "                        figsize=(26, len(cols)*4.2),\n",
    "                        gridspec_kw={'width_ratios' : [0.4, .3, .3]})\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "\n",
    "    ax = axs[i, 0]\n",
    "\n",
    "    sns.kdeplot(data=metrics_df[[col,'Model']],\n",
    "                x=col,\n",
    "                hue='Model',\n",
    "                warn_singular=False,\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title(f\"\\n{col}\", fontsize = 9)\n",
    "    ax.grid(visible=True, which = 'both', linestyle = '--', color='lightgrey', linewidth = 0.75)\n",
    "    ax.set(xlabel = '', ylabel = '')\n",
    "    \n",
    "    ax = axs[i, 1]\n",
    "\n",
    "    sns.boxplot(data=metrics_df,\n",
    "                x = 'Model',\n",
    "                y=col,\n",
    "                ax=ax,\n",
    "                width=0.25,\n",
    "                linewidth=0.90,\n",
    "                fliersize=2.25)\n",
    "    \n",
    "    ax.set(xlabel = '', ylabel = '')\n",
    "    ax.set_title(f\"{col}: Boxplot Grouped by Model\", fontsize = 9)\n",
    "\n",
    "    ax = axs[i, 2]\n",
    "\n",
    "    intervals = confidence_intervals[col]\n",
    "\n",
    "    for j, d in enumerate(intervals, start=1):\n",
    "\n",
    "        mean = d[0]\n",
    "        confidence_interval = (mean - d[1], mean + d[1])\n",
    "\n",
    "        # Plot the confidence interval as a line\n",
    "        ax.plot([j, j], confidence_interval, lw=1.5, color='blue')\n",
    "\n",
    "        # Plot the mean as a dot\n",
    "        ax.plot(j, mean, 'x', color='red')\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlim(0.5, 4.5)\n",
    "    ax.set_xticks([1, 2, 3, 4])\n",
    "    ax.set_xticklabels(['reinvent', 'crem', 'coati', 'safe'])\n",
    "    # axs[i, 2].sharey(axs[i, 1])\n",
    "    ax.set_title(f\"{col}: Mean and 95% Confidence Interval by Model\", fontsize = 9)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the models can be consistently differentiated. Some observations:\n",
    "\n",
    "* Sometimes SAFE produces distributions of molecules which do not pass filters. Why?\n",
    "* Confidence intervals are especially wide for certain models/metrics:\n",
    "    * CReM: Sphere Exclusion Diversity, Average Tanimoto Similarity, Average Cosine Similarity\n",
    "    * SAFE: Filtering scores\n",
    "* Confidence interval for REINVENT is especially narrow when considering functional group diversity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Filtering Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['Lipinski Score', 'Dundee Score', 'Lily Score']\n",
    "dataframes = [reinvent_df, crem_df, coati_df, safe_df]\n",
    "\n",
    "scores = {'reinvent' : [],\n",
    "          'crem' : [],\n",
    "          'coati' : [],\n",
    "          'safe' : []}\n",
    "\n",
    "for df, model in zip(dataframes, scores):\n",
    "\n",
    "    size = df['#'].sum()\n",
    "\n",
    "    for filter in filters:\n",
    "\n",
    "        filter_score = (df[filter] * df['#']).sum() / size\n",
    "\n",
    "        scores[model].append(filter_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bar plots of the scores color coded by model for each filter\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, filter in enumerate(['Lipinski', 'Dundee', 'Lily']):\n",
    "        \n",
    "    sns.barplot(x=models, y=[scores[model][i] for model in models], ax=ax[i], palette=default_palette)\n",
    "        \n",
    "    ax[i].set_title(filter)\n",
    "    ax[i].set_ylabel('Fraction of Molecules Passing Filter')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dfs = metrics_df.sort_values(['Avg_Filter_Score'])[:3]\n",
    "good_dfs = safe_df.sort_values(['Avg_Filter_Score'], ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (bad_dfs.index - 148).to_list()\n",
    "bad_dfs = [safe_distributions[i] for i in indices]\n",
    "\n",
    "bad_input_mols = []\n",
    "bad_mols = []\n",
    "all_mols = []\n",
    "\n",
    "for df in bad_dfs:\n",
    "\n",
    "    input_frag = df['Input Fragment'].to_list()[0]\n",
    "    input_mol = Chem.MolFromSmiles(input_frag)\n",
    "\n",
    "    three_smiles = df['SMILES'].sample(3).to_list()\n",
    "    three_mols = [Chem.MolFromSmiles(smile)for smile in three_smiles]\n",
    "\n",
    "    mols = [input_mol] + three_mols\n",
    "    all_mols.append(mols)\n",
    "\n",
    "MolsMatrixToGridImage(all_mols, subImgSize=(300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (good_dfs.index - 148).to_list()\n",
    "good_dfs = [safe_distributions[i] for i in indices]\n",
    "\n",
    "good_input_mols = []\n",
    "good_mols = []\n",
    "all_mols = []\n",
    "\n",
    "for df in good_dfs:\n",
    "\n",
    "    input_frag = df['Input Fragment'].to_list()[0]\n",
    "    input_mol = Chem.MolFromSmiles(input_frag)\n",
    "\n",
    "    three_smiles = df['SMILES'].sample(3).to_list()\n",
    "    three_mols = [Chem.MolFromSmiles(smile)for smile in three_smiles]\n",
    "\n",
    "    mols = [input_mol] + three_mols\n",
    "    all_mols.append(mols)\n",
    "\n",
    "MolsMatrixToGridImage(all_mols, subImgSize=(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = metrics_df['Model'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "\n",
    "ord = enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metrics_df[cols]\n",
    "\n",
    "X['Model'] = ord.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes feature correlation\n",
    "corr = X.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the correlation matrix\n",
    "labels = np.where(np.abs(corr)>0.75, \"S\",\n",
    "                  np.where(np.abs(corr)>0.5, \"M\",\n",
    "                           np.where(np.abs(corr)>0.25, \"W\", \"\")))\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr, mask=np.eye(len(corr)), square=True,\n",
    "            center=0, annot=True, fmt='.2f', linewidths=.5,\n",
    "            cmap=\"vlag\", cbar_kws={\"shrink\": 0.8});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's somewhat unsurprising that a lot of the variables are highly correlated, considering the fact that many of them measure similar quantities. Some nontrivial relationships:\n",
    "\n",
    " * High correlation between purchasability and internal diversity\n",
    " * High correlation between functional group and ring system diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a random forest to classify which model generated a given distribution based on the metrics calculated above. Outlined is the methodology:\n",
    "\n",
    "#### Initial Classification\n",
    "\n",
    "* Normalize FCD column using min-max scaler\n",
    "* Train random forest with 100 base estimators and balanced class weights; validate using 5-fold stratified cross-validation\n",
    "* Compute average precision, recall, and f1-score as well as standard deviations for each across 5 folds\n",
    "* Fit random forest on entire dataset to determine relative feature importances\n",
    "\n",
    "#### Classification w/ Feature Selection and Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(avg_precision, precision_std),\n",
    " (avg_recall, recall_std),\n",
    " (avg_f1, f1_std)] = make_predictions(metrics_df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Precision: {avg_precision:.3f}, Standard Deviation: {precision_std:.3f}\")\n",
    "print(f\"Average Recall: {avg_recall:.3f}, Standard Deviation: {recall_std:.3f}\")\n",
    "print(f\"Average F1: {avg_f1:.3f}, Standard Deviation: {f1_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metrics_df.loc[:, cols]\n",
    "y = metrics_df['Model'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "enc.fit_transform(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(cols, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification w/ Feature Selection and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = ['IntDiv1', 'FG', 'RS', 'FCD', 'SNN', 'Lipinski Score', 'Dundee Score', 'Lily Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(avg_precision, precision_std),\n",
    " (avg_recall, recall_std),\n",
    " (avg_f1, f1_std)] = make_predictions(metrics_df, cols2)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.3f}, Standard Deviation: {precision_std:.3f}\")\n",
    "print(f\"Average Recall: {avg_recall:.3f}, Standard Deviation: {recall_std:.3f}\")\n",
    "print(f\"Average F1: {avg_f1:.3f}, Standard Deviation: {f1_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metrics_df.loc[:, cols2]\n",
    "y = metrics_df['Model'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "enc.fit_transform(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(cols2, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3,\n",
    "                        ncols=1,\n",
    "                        figsize=(12, 3*4.2))\n",
    "\n",
    "col = 'Avg_Filter_Score'\n",
    "\n",
    "ax = axs[0]\n",
    "\n",
    "sns.kdeplot(data=metrics_df[[col,'Model']],\n",
    "            x=col,\n",
    "            hue='Model',\n",
    "            warn_singular=False,\n",
    "            ax=ax)\n",
    "    \n",
    "ax.set_title(f\"\\n{col}\", fontsize = 9)\n",
    "ax.grid(visible=True, which = 'both', linestyle = '--', color='lightgrey', linewidth = 0.75)\n",
    "ax.set(xlabel = '', ylabel = '')\n",
    "    \n",
    "ax = axs[1]\n",
    "\n",
    "sns.boxplot(data=metrics_df,\n",
    "            x = 'Model',\n",
    "            y=col,\n",
    "            ax=ax,\n",
    "            width=0.25,\n",
    "            linewidth=0.90,\n",
    "            fliersize=2.25)\n",
    "    \n",
    "ax.set(xlabel = '', ylabel = '')\n",
    "ax.set_title(f\"{col}: Boxplot Grouped by Model\", fontsize = 9)\n",
    "\n",
    "ax = axs[2]\n",
    "\n",
    "intervals = confidence_intervals[col]\n",
    "\n",
    "for j, d in enumerate(intervals, start=1):\n",
    "\n",
    "    mean = d[0]\n",
    "    confidence_interval = (mean - d[1], mean + d[1])\n",
    "\n",
    "    # Plot the confidence interval as a line\n",
    "    ax.plot([j, j], confidence_interval, lw=1.5, color='blue')\n",
    "\n",
    "    # Plot the mean as a dot\n",
    "    ax.plot(j, mean, 'x', color='red')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_xticks([1, 2, 3, 4])\n",
    "ax.set_xticklabels(['reinvent', 'crem', 'coati', 'safe'])\n",
    "# axs[i, 2].sharey(axs[i, 1])\n",
    "ax.set_title(f\"{col}: Mean and 95% Confidence Interval by Model\", fontsize = 9)\n",
    "    \n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['IntDiv1', 'FG', 'Avg_Filter_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_df1 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'reinvent']['IntDiv1']})\n",
    "set1_df2 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'crem']['IntDiv1']})\n",
    "set1_df3 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'coati']['IntDiv1']})\n",
    "set1_df4 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'safe']['IntDiv1']})\n",
    "\n",
    "set2_df1 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'reinvent']['FG']})\n",
    "set2_df2 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'crem']['FG']})\n",
    "set2_df3 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'coati']['FG']})\n",
    "set2_df4 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'safe']['FG']})\n",
    "\n",
    "set3_df1 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'reinvent']['Avg_Filter_Score']})\n",
    "set3_df2 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'crem']['Avg_Filter_Score']})\n",
    "set3_df3 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'coati']['Avg_Filter_Score']})\n",
    "set3_df4 = pd.DataFrame({'Score': metrics_df[metrics_df['Model'] == 'safe']['Avg_Filter_Score']})\n",
    "\n",
    "# Combine dataframes into a single dataframe with labels\n",
    "df1 = pd.concat([set1_df1.assign(Metric='Internal Diversity', Model='reinvent'),\n",
    "                 set1_df2.assign(Metric='Internal Diversity', Model='crem'),\n",
    "                 set1_df3.assign(Metric='Internal Diversity', Model='coati'),\n",
    "                 set1_df4.assign(Metric='Internal Diversity', Model='safe')],\n",
    "                ignore_index=True)\n",
    "\n",
    "df2 = pd.concat([set2_df1.assign(Metric='Functional Group Diversity', Model='reinvent'),\n",
    "                 set2_df2.assign(Metric='Functional Group Diversity', Model='crem'),\n",
    "                 set2_df3.assign(Metric='Functional Group Diversity', Model='coati'),\n",
    "                 set2_df4.assign(Metric='Functional Group Diversity', Model='safe')],\n",
    "                ignore_index=True)\n",
    "\n",
    "df3 = pd.concat([set3_df1.assign(Metric='Average Filter Score', Model='reinvent'),\n",
    "                 set3_df2.assign(Metric='Average Filter Score', Model='crem'),\n",
    "                 set3_df3.assign(Metric='Average Filter Score', Model='coati'),\n",
    "                 set3_df4.assign(Metric='Average Filter Score', Model='safe')],\n",
    "                ignore_index=True)\n",
    "\n",
    "# Combine all into one dataframe\n",
    "all_data = pd.concat([df1, df2, df3])\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxenplot(x='Metric', y='Score', hue='Model', data=all_data, palette=default_palette, k_depth='full')\n",
    "# plt.title('Grouped Boxplot of Value Distributions by Set')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, .6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['FCD'] = metrics_df['FCD'] / metrics_df['FCD'].max()\n",
    "cols.remove('Avg_Filter_Score')\n",
    "cols.remove('Filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df = metrics_df[['Model'] + cols].groupby(\"Model\").std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_reinvent = (std_df.loc['reinvent']**2).mean()\n",
    "var_crem = (std_df.loc['crem']**2).mean()\n",
    "var_coati = (std_df.loc['coati']**2).mean()\n",
    "var_safe = (std_df.loc['safe']**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bar plots of the scores color coded by model for each filter\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "\n",
    "sns.barplot(x=models, y=[var_reinvent, var_crem, var_coati, var_safe], ax=ax, palette=default_palette)\n",
    "        \n",
    "ax.set_ylabel('Average Variance across Metrics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinvent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
